#!/bin/bash
# ============================================================================
# GraphRAG Security Architect - Docker Entrypoint
# ============================================================================

set -e

echo "=============================================="
echo "ðŸš€ GraphRAG Security Architect - Starting..."
echo "=============================================="

# -----------------------------------------------------------------------------
# Environment Info
# -----------------------------------------------------------------------------
echo ""
echo "ðŸ“‹ Configuration:"
echo "   - Knowledge Graph: ${ENABLE_KNOWLEDGE_GRAPH:-false}"
echo "   - Entity Extraction: ${ENABLE_LLM_ENTITY_EXTRACTION:-false}"
echo "   - LLM Model: ${LLM_MODEL:-llama3.2}"
echo "   - Embedding Model: ${EMBEDDING_MODEL:-mxbai-embed-large}"
echo "   - Ollama Host: ${OLLAMA_HOST:-http://host.docker.internal:11434}"
echo ""

# -----------------------------------------------------------------------------
# Wait for Ollama (if configured)
# -----------------------------------------------------------------------------
OLLAMA_URL="${OLLAMA_HOST:-http://host.docker.internal:11434}"

echo "ðŸ” Checking Ollama at ${OLLAMA_URL}..."

MAX_RETRIES=30
RETRY_COUNT=0

while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
    if curl -s "${OLLAMA_URL}/api/tags" > /dev/null 2>&1; then
        echo "âœ… Ollama is available!"
        break
    fi
    
    RETRY_COUNT=$((RETRY_COUNT + 1))
    echo "   Waiting for Ollama... (${RETRY_COUNT}/${MAX_RETRIES})"
    sleep 2
done

if [ $RETRY_COUNT -eq $MAX_RETRIES ]; then
    echo "âš ï¸  Warning: Could not connect to Ollama at ${OLLAMA_URL}"
    echo "   Make sure Ollama is running and accessible."
    echo "   The application will start but may fail on first query."
fi

# -----------------------------------------------------------------------------
# Check for required models
# -----------------------------------------------------------------------------
echo ""
echo "ðŸ” Checking for required models..."

check_model() {
    local model=$1
    if curl -s "${OLLAMA_URL}/api/tags" 2>/dev/null | grep -q "\"name\":\"${model}\""; then
        echo "   âœ… Model '${model}' is available"
        return 0
    else
        echo "   âš ï¸  Model '${model}' not found. Pull it with: ollama pull ${model}"
        return 1
    fi
}

check_model "${LLM_MODEL:-llama3.2}" || true
check_model "${EMBEDDING_MODEL:-mxbai-embed-large}" || true

# -----------------------------------------------------------------------------
# Wait for Neo4j (if Knowledge Graph enabled)
# -----------------------------------------------------------------------------
if [ "${ENABLE_KNOWLEDGE_GRAPH}" = "true" ]; then
    NEO4J_URL="${NEO4J_URI:-bolt://neo4j:7687}"
    echo ""
    echo "ðŸ” Checking Neo4j at ${NEO4J_URL}..."
    
    RETRY_COUNT=0
    while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
        if nc -z "${NEO4J_URL#bolt://}" 2>/dev/null; then
            echo "âœ… Neo4j is available!"
            break
        fi
        
        RETRY_COUNT=$((RETRY_COUNT + 1))
        echo "   Waiting for Neo4j... (${RETRY_COUNT}/${MAX_RETRIES})"
        sleep 2
    done
fi

# -----------------------------------------------------------------------------
# Create .env if not exists
# -----------------------------------------------------------------------------
if [ ! -f /app/.env ]; then
    echo ""
    echo "ðŸ“ Creating .env from environment variables..."
    cat > /app/.env << EOF
# Auto-generated by docker-entrypoint.sh
ENABLE_KNOWLEDGE_GRAPH=${ENABLE_KNOWLEDGE_GRAPH:-false}
ENABLE_LLM_ENTITY_EXTRACTION=${ENABLE_LLM_ENTITY_EXTRACTION:-false}
LLM_MODEL=${LLM_MODEL:-llama3.2:latest}
EMBEDDING_MODEL=${EMBEDDING_MODEL:-mxbai-embed-large:latest}
NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
NEO4J_USER=${NEO4J_USER:-neo4j}
NEO4J_PASSWORD=${NEO4J_PASSWORD:-password}
OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11434}
EOF
fi

# -----------------------------------------------------------------------------
# Set Ollama host for langchain
# -----------------------------------------------------------------------------
export OLLAMA_HOST="${OLLAMA_HOST:-http://host.docker.internal:11434}"

# -----------------------------------------------------------------------------
# Start Application
# -----------------------------------------------------------------------------
echo ""
echo "=============================================="
echo "ðŸŽ¯ Starting GraphRAG Security Architect..."
echo "   Access at: http://localhost:5000"
echo "=============================================="
echo ""

# Execute the command passed to the container
exec "$@"
