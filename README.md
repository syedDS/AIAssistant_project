# ğŸ“ AI-Assistant-Self Tutoring

A **self-tutoring AI assistant** with document grounding, knowledge graphs, and deep research capabilities. Upload your learning materials and get intelligent, cited answers from your documents.
<img width="929" height="440" alt="image" src="https://github.com/user-attachments/assets/9a3ffef4-962c-441b-90e4-85bd6be3efa4" />

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-3.0+-green.svg)
![License](https://img.shields.io/badge/License-MIT-orange.svg)

---

## âœ¨ Key Features

- ğŸ“„ **Document-Grounded Answers** - Responses based ONLY on your uploaded documents
- ğŸ” **Hybrid RAG Search** - Combines vector similarity + knowledge graph traversal
- ğŸ•¸ï¸ **Knowledge Graph** - Builds entity relationships using Neo4j (optional)
- ğŸ”¬ **Deep Research** - Web search + document synthesis
- ğŸ¤– **Local LLM** - Uses Ollama (no API keys needed)
- ğŸ›¡ï¸ **Security Guardrails** - Configurable content filtering and safety checks
- ğŸ’¾ **Large File Support** - Up to 50MB per document

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.10+**
- **Ollama** - [Download here](https://ollama.com)
- **Neo4j** (optional) - For knowledge graph features
- **Docker** (optional) - For containerized deployment

### Installation

**Linux/macOS:**
```bash
# Clone the repository
git clone <repository-url>
cd graphrag_project

# Make scripts executable
chmod +x startup.sh check_models.sh check_indexing.sh upgrade_llm.sh

# Run the startup script (handles everything automatically)
./startup.sh
```

**Windows:**
```batch
REM Clone the repository
git clone <repository-url>
cd graphrag_project

REM Run the startup script
startup.bat
```

The startup script will:
1. âœ… Check system requirements
2. âœ… Verify Ollama is running
3. âœ… Check/download required models
4. âœ… Install Python dependencies
5. âœ… Start the application

**Access the application at:** http://localhost:5000

---

## ğŸ“– Usage

### Basic Workflow

1. **Start the Application**
   ```bash
   ./startup.sh
   ```

2. **Upload Documents**
   - Go to http://localhost:5000
   - Upload PDFs, DOCX, TXT, or other supported formats
   - Maximum size: 50MB per file

3. **Ask Questions**
   - Type your question in the chat interface
   - Get answers grounded in your documents
   - See source citations for each answer

### Supported File Formats

- ğŸ“„ PDF
- ğŸ“ DOCX
- ğŸ“ƒ TXT, MD
- ğŸ“Š CSV, JSON
- ğŸŒ HTML, XML

---

## âš™ï¸ Configuration

### Operating Modes

**Fast Mode (Default)** - Vector search only, no knowledge graph
```bash
./startup.sh --fast
```

**Full Mode** - With knowledge graph (requires Neo4j)
```bash
./startup.sh --full
```

**Docker Mode** - Containerized deployment
```bash
./startup.sh --docker
```

### Environment Variables

Create a `.env` file (copy from `.env.example`):

```bash
# LLM Configuration
LLM_MODEL=llama3.2:3b                 # Recommended: 3b or 8b variant
EMBEDDING_MODEL=nomic-embed-text      # Most reliable option

# Ollama Connection
OLLAMA_HOST=http://localhost:11434

# Optional: Neo4j (for knowledge graph)
# ENABLE_KNOWLEDGE_GRAPH=true
# NEO4J_URI=bolt://localhost:7687
# NEO4J_USER=neo4j
# NEO4J_PASSWORD=your-password
```

### Model Recommendations

| Model | Size | RAM Required | Best For | Performance |
|-------|------|--------------|----------|-------------|
| `llama3.2:3b` | 2GB | 4GB+ | Balanced use | â­â­â­â­ Recommended |
| `llama3:8b` | 5GB | 8GB+ | High accuracy | â­â­â­â­â­ Best quality |
| `qwen2.5:7b` | 4GB | 8GB+ | Technical docs | â­â­â­â­â­ |
| `phi3:3.8b` | 2.3GB | 4GB+ | Low resources | â­â­â­ |

**Upgrade your LLM:**
```bash
./upgrade_llm.sh
```

### Embedding Models

| Model | Size | Speed | Reliability |
|-------|------|-------|-------------|
| `nomic-embed-text` | 700MB | Fast | âœ… Most reliable |
| `all-minilm` | 80MB | Very Fast | âœ… Very reliable |
| `mxbai-embed-large` | 1.5GB | Slower | âš ï¸ May have detection issues |

---

## ğŸ› ï¸ Utility Scripts

### check_models.sh
Verify Ollama models are installed and working:
```bash
./check_models.sh          # Check models
./check_models.sh --pull   # Auto-pull missing models
```

### check_indexing.sh
Diagnose document indexing issues:
```bash
./check_indexing.sh
```

Shows:
- Documents in `data_store/`
- ChromaDB indexing status
- Test search results
- Current configuration

### upgrade_llm.sh
Upgrade to a better LLM model:
```bash
./upgrade_llm.sh
```

Features:
- Auto-detects system RAM
- Recommends best model for your system
- Downloads and configures automatically

---

## ğŸ”§ Troubleshooting

### Common Issues

#### 1. Model Not Found After Pulling

**Problem:** Ran `ollama pull <model>` but model still shows 404 errors

**Solution:**
```bash
# Restart Ollama service
pkill ollama && ollama serve &

# Verify model appears
ollama list
```

#### 2. Documents Not Found in Search

**Problem:** Documents are indexed but queries return "Not found in your documents"

**Cause:** LLM model is too weak (e.g., llama3.2:1b)

**Solution:**
```bash
# Upgrade to 3b or 8b variant
./upgrade_llm.sh
```

#### 3. ChromaDB Reset After Model Change

**Problem:** Changing embedding models wipes the database

**Solution:**
```bash
# Stick with one embedding model, or re-index
./startup.sh
# Wait for documents to re-index automatically
```

For detailed troubleshooting, see [TROUBLESHOOTING_INDEXING.md](TROUBLESHOOTING_INDEXING.md).

---

## ğŸ³ Docker Deployment

### Quick Start

```bash
# Fast Mode (uses host Ollama)
docker-compose up -d graphrag

# Full Mode (with Neo4j knowledge graph)
docker-compose --profile kg up -d

# With containerized Ollama (GPU)
docker-compose --profile ollama up -d
```

### Environment Configuration

```bash
# Create .env file
cp .env.example .env

# Edit configuration
nano .env
```

---

## ğŸ“š API Reference

### Core Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/ask` | POST | Ask a question |
| `/upload` | POST | Upload document |
| `/deep-research` | POST | Web research + synthesis |
| `/config-status` | GET | Get configuration |

### Diagnostic Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/chroma-status` | GET | Index statistics |
| `/debug-search` | POST | Test vector search |
| `/graph-stats` | GET | Knowledge graph stats |
| `/data-store-files` | GET | List indexed files |

### Example: Ask Question

```bash
curl -X POST http://localhost:5000/ask \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What are the security best practices?",
    "mode": "hybrid"
  }'
```

### Example: Upload Document

```bash
curl -X POST http://localhost:5000/upload \
  -F "file=@document.pdf"
```

---

## ğŸ“ Project Structure

```
graphrag_project/
â”œâ”€â”€ graphrag_app.py          # Main Flask application
â”œâ”€â”€ config.py                # Configuration with auto-detection
â”œâ”€â”€ search.py                # Hybrid RAG search
â”œâ”€â”€ document_processor.py    # Document parsing & chunking
â”œâ”€â”€ deep_research.py         # Web research functionality
â”œâ”€â”€ guardrails_handler.py    # Security guardrails
â”‚
â”œâ”€â”€ entity_extractor.py      # LLM-based entity extraction
â”œâ”€â”€ entity_resolver.py       # Entity deduplication
â”œâ”€â”€ neo4j_graph.py           # Knowledge graph operations
â”œâ”€â”€ ontology.py              # Entity schemas
â”‚
â”œâ”€â”€ startup.sh               # Linux/macOS startup script
â”œâ”€â”€ startup.bat              # Windows startup script
â”œâ”€â”€ check_models.sh          # Model verification
â”œâ”€â”€ check_indexing.sh        # Indexing diagnostics
â”œâ”€â”€ upgrade_llm.sh           # LLM upgrade helper
â”‚
â”œâ”€â”€ Dockerfile               # Container build
â”œâ”€â”€ docker-compose.yml       # Multi-service orchestration
â”œâ”€â”€ docker-entrypoint.sh     # Container startup
â”‚
â”œâ”€â”€ templates/               # Web UI templates
â”œâ”€â”€ guardrails/              # Guardrails configuration
â”‚
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ .env.example             # Environment template
â””â”€â”€ README.md                # This file
```

---

## ğŸ”„ Advanced Features

### Knowledge Graph Mode

Enable entity extraction and relationship mapping:

```bash
# Start with knowledge graph
./startup.sh --full

# Or enable via API
curl -X POST http://localhost:5000/config/enable-kg
```

### Deep Research

Perform web-based research with document synthesis:

```bash
curl -X POST http://localhost:5000/deep-research \
  -H "Content-Type: application/json" \
  -d '{
    "topic": "Machine learning best practices",
    "include_web": true,
    "include_docs": true,
    "depth": "standard"
  }'
```

**Depth Levels:**
- `quick` - ~5 sources, 30 seconds
- `standard` - ~15 sources, 1 minute
- `deep` - ~25+ sources, 2 minutes

### Configurable Search Parameters

```bash
curl -X POST http://localhost:5000/config/search-params \
  -H "Content-Type: application/json" \
  -d '{
    "top_k": 8,
    "min_relevance": 0.4,
    "search_mode": "hybrid",
    "context_window": 8000
  }'
```

---

## ğŸ“ Model Variant Auto-Detection

The application automatically detects model variants installed in Ollama:

- âœ… `.env` specifies `LLM_MODEL=llama3.2`
- âœ… You have `llama3.2:3b` installed
- âœ… App auto-detects and uses `llama3.2:3b`

**Console Output:**
```
ğŸ¤– Initializing LLM (llama3.2)...
â„¹ï¸  Auto-detected model variant: llama3.2:3b (configured: llama3.2)
âœ… LLM ready
```

**No configuration needed** - it just works!

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“„ License

MIT License - See LICENSE file for details.

---

## ğŸ™ Acknowledgments

Built with:
- [Ollama](https://ollama.com) - Local LLM runtime
- [LangChain](https://langchain.com) - LLM orchestration
- [ChromaDB](https://www.trychroma.com) - Vector database
- [Neo4j](https://neo4j.com) - Graph database
- [Flask](https://flask.palletsprojects.com) - Web framework

---

**Built with â¤ï¸ for self-directed learners**
