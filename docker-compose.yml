# ============================================================================
# GraphRAG Security Architect - Docker Compose
# ============================================================================
# 
# Quick Start (Vector Search Only - Fast Mode):
#   docker-compose up -d graphrag
#
# Full Mode (with Knowledge Graph):
#   docker-compose --profile kg up -d
#
# With Ollama in container:
#   docker-compose --profile ollama up -d
#
# All services:
#   docker-compose --profile kg --profile ollama up -d
#
# ============================================================================

version: '3.8'

services:
  # ==========================================================================
  # GraphRAG Security Architect - Main Application
  # ==========================================================================
  graphrag:
    build:
      context: .
      dockerfile: Dockerfile
    image: graphrag-security:latest
    container_name: graphrag-security
    ports:
      - "5000:5000"
    environment:
      # Feature Flags
      - ENABLE_KNOWLEDGE_GRAPH=${ENABLE_KNOWLEDGE_GRAPH:-false}
      - ENABLE_LLM_ENTITY_EXTRACTION=${ENABLE_LLM_ENTITY_EXTRACTION:-false}
      
      # LLM Configuration
      - LLM_MODEL=${LLM_MODEL:-llama3.2}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-mxbai-embed-large}
      - OLLAMA_HOST=${OLLAMA_HOST:-http://host.docker.internal:11434}
      
      # Neo4j Configuration (used if KG enabled)
      - NEO4J_URI=${NEO4J_URI:-bolt://neo4j:7687}
      - NEO4J_USER=${NEO4J_USER:-neo4j}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-graphrag123}
      
      # Flask
      - FLASK_ENV=${FLASK_ENV:-production}
    volumes:
      # Persistent document storage
      - ./data_store:/app/data_store
      # Persistent vector database
      - ./chroma_graphrag_db:/app/chroma_graphrag_db
      # Persistent file tracker
      - ./indexed_files.json:/app/indexed_files.json
    networks:
      - graphrag-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/config-status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # For Mac/Windows to access host's Ollama
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # ==========================================================================
  # Neo4j - Knowledge Graph Database (Optional)
  # ==========================================================================
  neo4j:
    image: neo4j:5.15-community
    container_name: graphrag-neo4j
    profiles:
      - kg
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-graphrag123}
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=1G
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    networks:
      - graphrag-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:7474"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ==========================================================================
  # Ollama - LLM Service (Optional - use if not running locally)
  # ==========================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: graphrag-ollama
    profiles:
      - ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - graphrag-network
    restart: unless-stopped
    # Pull models on startup
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 10
        ollama pull llama3.2
        ollama pull mxbai-embed-large
        wait
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Remove GPU section if no GPU available

  # ==========================================================================
  # Ollama CPU-only version (for systems without GPU)
  # ==========================================================================
  ollama-cpu:
    image: ollama/ollama:latest
    container_name: graphrag-ollama-cpu
    profiles:
      - ollama-cpu
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - graphrag-network
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        ollama serve &
        sleep 10
        ollama pull llama3.2
        ollama pull mxbai-embed-large
        wait

# ============================================================================
# Networks
# ============================================================================
networks:
  graphrag-network:
    driver: bridge

# ============================================================================
# Volumes
# ============================================================================
volumes:
  neo4j_data:
    driver: local
  neo4j_logs:
    driver: local
  ollama_data:
    driver: local
